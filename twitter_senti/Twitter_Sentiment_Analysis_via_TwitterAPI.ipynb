{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1121042d",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93581b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below two functions were modified from \n",
    "https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Recent-Search/recent_search.py:\n",
    "\n",
    "1. bearer_oauth(r)\n",
    "2. connect_to_endpoint(url, params)\n",
    "\n",
    "Others were written by Yi-ching, Hung \n",
    "\n",
    "Last updated: 2022-Jan-24th\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#collect, process, store data\n",
    "import requests\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "bearer_token = '(bearer token)'\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "#the target brands \n",
    "brand_ls = [\"Moderna\",\"AstraZeneca\",\"Pfizer\"]\n",
    "\n",
    "\n",
    "#get data from Twitter API:\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    \"\"\"\n",
    "    get response from endpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\n",
    "def get(start, end):\n",
    "    \"\"\"\n",
    "    get tweets data using params\n",
    "    start/end: start/end time, example format: '2021-12-30T00:00:00Z'\n",
    "    \n",
    "    \"\"\"\n",
    "    #dictionary containing three brands\n",
    "    j_d=dict()\n",
    "    \n",
    "    for brand in brand_ls: #brand_ls = [\"Moderna\",\"AstraZeneca\",\"Pfizer\"]\n",
    "        j_d[brand]=[]\n",
    "        \n",
    "    \n",
    "    for brand in brand_ls: \n",
    "        \n",
    "        query_params = {'query': f'({brand} OR #{brand} OR #{brand.lower()} OR {brand.lower()}) lang:en -is:retweet',\n",
    "                        'max_results': '100','tweet.fields':'created_at','start_time':f'{start}',\n",
    "                        'end_time':f'{end}'}\n",
    "        \n",
    "        json_response = connect_to_endpoint(search_url, query_params)\n",
    "        \n",
    "        j_d[brand].append(json_response)\n",
    "\n",
    "    return j_d \n",
    "\n",
    "\n",
    "#data preprocessing\n",
    "def data_prepross(j_d): #j_d = json dict gotten from get()\n",
    "    \n",
    "    #create dict to contain target twitter data\n",
    "    text_d = dict()\n",
    "    for brand in brand_ls:\n",
    "        text_d[brand] = [[],[],[],[]] #4 target data\n",
    "\n",
    "    \n",
    "    for brand in j_d.keys():\n",
    "        for d in j_d[brand][0]['data']: # for d in a list of dictionaries(each is one tweet)\n",
    "            \n",
    "            #text_d[brand][0] data to be processed, [1] raw data,[2]date,[3]id\n",
    "            text_d[brand][0].append(d['text'])  #append a tweet string at a time: list of tweet strings\n",
    "            text_d[brand][1].append(d['text'])\n",
    "            text_d[brand][2].append(d['created_at'])\n",
    "            text_d[brand][3].append(d['id'])\n",
    "\n",
    "    #take out urls, #, and @\n",
    "    for brand in text_d.keys():#three brands\n",
    "        for i in range(len(text_d[brand][0])): #the to-be-processed text list\n",
    "            txt = text_d[brand][0][i] #for each string in text list e.g.,\"Moderna vaccine\":[[s,s,s,s],[],[],[]]\n",
    "            txt2=re.sub('https://\\S+|@\\S+|#\\S+|&amp;','', txt)\n",
    "            txt3=re.sub('\\n',' ', txt2) #convert \\n to space\n",
    "            text_d[brand][0][i] = txt3 #\"Moderna vaccine\":[[txt],[],[],[]]\n",
    "    \n",
    "    \n",
    "    #expand         \n",
    "    for brand in text_d.keys(): #three brands\n",
    "        for i in range(len(text_d[brand][0])): #for each tweet in text list for a brand            \n",
    "            line = text_d[brand][0][i]\n",
    "            \n",
    "            expanded_words = []\n",
    "            for word in line.split(): #to fix each word in a list of words from a line\n",
    "                \n",
    "                # using contractions.fix to expand the shortened words\n",
    "                expanded_words.append(contractions.fix(word))  #save fixed words in a list \n",
    "            \n",
    "            fixed_line = ' '.join(expanded_words) #convert the list back to fixed line           \n",
    "            text_d[brand][0][i] = fixed_line \n",
    "\n",
    "    \n",
    "    #remove duplicates (change duplicates to '')\n",
    "    dupli_d={}\n",
    "    for brand in text_d.keys(): \n",
    "        count=0\n",
    "        print('total number of tweet for ',brand, ': ', len(text_d[brand][0]),'\\n') #how many strings in the list\n",
    "        \n",
    "        for i in range(len(text_d[brand][0])):#len of a brand list(the number of tweets)\n",
    "            line = text_d[brand][0][i] #a string in the list\n",
    "            \n",
    "            if line in dupli_d.keys():\n",
    "                count +=1\n",
    "                text_d[brand][0][i]='' #turn a duplicated string in the text list to empty string\n",
    "                text_d[brand][1][i]='' #turn a duplicated string in the raw list to empty string          \n",
    "            if line not in dupli_d:\n",
    "                dupli_d[line]=1\n",
    " \n",
    "    \n",
    "    #remove len <= 2 tweets (including duplicates)\n",
    "    res_d=dict()\n",
    "    for brand in brand_ls:#brand_ls = [\"Moderna\",\"AstraZeneca\",\"Pfizer\"]\n",
    "        res_d[brand]=[[],[],[],[]]\n",
    "    \n",
    "    for brand in text_d.keys(): #3 brands\n",
    "        count=0\n",
    "\n",
    "        for i in range(len(text_d[brand][0])): #range(number of tweets for a brand)\n",
    "            line = text_d[brand][0][i]\n",
    "            raw = text_d[brand][1][i]\n",
    "            date = text_d[brand][2][i]\n",
    "            uid = text_d[brand][3][i]\n",
    "\n",
    "            if len(line.split())>2: #if a string has more than two words\n",
    "                res_d[brand][0].append(line) #text to be processed\n",
    "                res_d[brand][1].append(raw) #raw text\n",
    "                res_d[brand][2].append(date)\n",
    "                res_d[brand][3].append(uid)\n",
    "            else:\n",
    "                count+=1\n",
    "                \n",
    "        print('No enough content or duplicates: ', brand,' ', count)\n",
    "        print('Total tweets left: ', len(res_d[brand][0]),'\\n')\n",
    "                \n",
    "\n",
    "    return res_d #return a dictionary: {processed data, raw data, date, uid}\n",
    "     \n",
    "\n",
    "\n",
    "#sentiment analysis:\n",
    "def to_sent_df(dic, date, sentiment_df): \n",
    "    \"\"\"\n",
    "    1. check if newly-collected tweets are in existing df before processing\n",
    "    2. do senti analysis to get a score, and append to senti_df\n",
    "    dic = res_d\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(sentiment_df) #sentiment analysis df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        answer = input('File not found, create a new one in current directory? Y or N')\n",
    "        if answer.lower() == 'y':\n",
    "            df = pd.DataFrame(columns=['brand','date','senti','score','text',\n",
    "                                   'raw_text','uid','created_datetime'])\n",
    "        else:\n",
    "            print('A file has to be created before running this function')\n",
    "            return 'A file has to be created before running this function'\n",
    "\n",
    "    sent_d = {'neutral':{'count':0, 'data':[]},\n",
    "              'negative':{'count':0, 'data':[]},\n",
    "              'positive':{'count':0, 'data':[]}} #dict with sentiment, count pos,neu,neg number\n",
    "\n",
    "    #check duplication\n",
    "    for brand in dic.keys(): \n",
    "        #{\"Moderna vaccine\":[[s,s,s],[],[],[]],\"AstraZeneca vaccine\":[[],[],[],[]],\"Pfizer vaccine\":[[],[],[],[]]\n",
    "        \n",
    "        count_old=0\n",
    "        count_new=0\n",
    "        \n",
    "        for i in range(len(dic[brand][0])): #for each string index in string list for a brand\n",
    "            s = dic[brand][0][i]  #a tweet string\n",
    "            raw = dic[brand][1][i]\n",
    "            created_datetime = dic[brand][2][i] #created_date \n",
    "            uid = dic[brand][3][i] #user id\n",
    "            \n",
    "            if s.lower() in df[df['brand']==brand].text.values: #check if string is in the existing df   \n",
    "                count_old+=1\n",
    "\n",
    "                \n",
    "            elif s.lower() not in df[df['brand']==brand].text.values:#if string is new, continue\n",
    "                count_new+=1\n",
    "                \n",
    "                d = sia.polarity_scores(s) #score for a sentence\n",
    "                num_sum = d['compound']\n",
    "\n",
    "                if num_sum < 0.05 and num_sum > (-0.05):\n",
    "                    senti='neutral'\n",
    "                    sent_d['neutral']['count'] += 1\n",
    "                \n",
    "                elif num_sum < (-0.05):\n",
    "                    senti='negative'\n",
    "                    sent_d['negative']['count'] += 1\n",
    "                \n",
    "                elif num_sum > 0.05:\n",
    "                    senti='positive'\n",
    "                    sent_d['positive']['count'] += 1             \n",
    "                \n",
    "                df = df.append({'brand':brand,'date':date,'senti':senti,'score':num_sum,'text':s.lower(),\n",
    "                               'raw_text':raw,'uid':uid,'created_datetime':created_datetime},ignore_index=True)\n",
    "        print(brand,'duplicated in senti data: ', count_old, ' new: ', count_new, '\\n\\n')\n",
    "    print('df_senti tail:\\n', df.tail())\n",
    "    df.to_csv(sentiment_df, index=False) # save to sentiment analysis df with scores\n",
    "    print(f'\\ndata saved to dataframe {sentiment_df}')\n",
    "    \n",
    "\n",
    "\n",
    "def main(): \n",
    "    \"\"\"\n",
    "    Get data, process, analyze, store data\n",
    "    \n",
    "    need to specify start/end time, and the dataframe name\n",
    "    \n",
    "    \"\"\"\n",
    " \n",
    "    \n",
    "    ##### specify start/end time, and the dataframe name here: ####\n",
    "    \n",
    "    starttime = datetime.datetime.utcnow()- datetime.timedelta(hours=24)\n",
    "    endtime = starttime + datetime.timedelta(hours=23)\n",
    "    sentiment_df = 'sentiment_df'\n",
    "    \n",
    "    #############################\n",
    "    \n",
    "    start = starttime.isoformat(timespec='seconds')+'.000Z'\n",
    "    end = endtime.isoformat(timespec='seconds')+'.000Z'\n",
    "    print('StartTime: ',start)\n",
    "    print('EndTime: ',end)    \n",
    "    date = start.split('T')[0]        \n",
    "    \n",
    "    #start running\n",
    "    get_d = get(start,end)\n",
    "    preprossed_d = data_prepross(get_d) \n",
    "    to_sent_df(preprossed_d,date,sentiment_df )\n",
    "    \n",
    "\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0365478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "class Sentiment_df_viz():\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Visualisation of sentiment_df\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,df, startdate = None, enddate= None):\n",
    "        \"\"\"\n",
    "        startdate: specify a date to get data on and after that date\n",
    "        \n",
    "        date example: '2022-01-04'\n",
    "        \n",
    "        \"\"\"\n",
    "        self.df =pd.read_csv(df)\n",
    "        \n",
    "        if (startdate != None) and (enddate != None):\n",
    "            self.df = self.df[(self.df['date'] >= startdate) & (self.df['date'] <= enddate) ]\n",
    "        elif startdate != None:\n",
    "            self.df = self.df[self.df['date'] >= startdate]\n",
    "        elif enddate != None:\n",
    "            self.df = self.df[self.df['date'] <= enddate]\n",
    "    \n",
    "    def df_sum(self):\n",
    "        \"\"\"\n",
    "        get stats for each brand\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df1 = self.df\n",
    "\n",
    "        #count numbers of senti and change colnams to 'number'\n",
    "        df_count = df1.groupby(['brand','date','senti']).count().reset_index()\n",
    "        df_count = df_count.rename(columns={'score':'number'})\n",
    "\n",
    "        #group by again to get daily percentage \n",
    "        df_sum = df_count.groupby(['brand','date','senti']).sum()\n",
    "\n",
    "        #get percentage\n",
    "        df_sum['percentage']= df_sum.groupby(level=(0,1))['number'].apply(lambda x: x*100/x.sum())\n",
    "        #print('\\n\\n Daily percentage:\\n ',df_sum.tail(3))\n",
    "\n",
    "        #reset to sort values\n",
    "        df_sum = df_sum.reset_index()\n",
    "\n",
    "        results =df_sum.groupby(['brand','date']).sum().reset_index()\n",
    "\n",
    "        return df_sum        \n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"\n",
    "        print out the number of tweets per date for each brand\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df_sum = self.df_sum()\n",
    "\n",
    "\n",
    "        results =df_sum.groupby(['brand','date']).sum().reset_index()\n",
    "        print('\\n\\n Sum for brand and date:\\n ')\n",
    "        print('AstraZeneca: \\n',results[results['brand']=='AstraZeneca'][['date','text']])\n",
    "        print('Pfizer: \\n',results[results['brand']=='Pfizer'][['date','text']])\n",
    "        print('Moderna: \\n',results[results['brand']=='Moderna'][['date','text']])\n",
    "\n",
    "        print('\\n\\n Total tweets ',df_sum.number.sum())\n",
    "\n",
    "    \n",
    "    def line_plots(self):\n",
    "        \"\"\"\n",
    "        take the output of print_stats as variable\n",
    "        \"\"\"\n",
    "        df = self.df_sum()\n",
    "        brandls = ['AstraZeneca','Moderna','Pfizer']\n",
    "        brandnum = len(brandls)\n",
    "        fig, axes = plt.subplots(brandnum,1, figsize=(15,18),dpi=100)\n",
    "        fig.tight_layout(pad=12.0)\n",
    "\n",
    "        for i in range(len(brandls)):\n",
    "            A = df[df['brand']== brandls[i]]\n",
    "            axes[i].set_title(f'{brandls[i]}: Sentiment Change over Time', fontsize=20)\n",
    "            #sns.scatterplot(ax=axes[0],x=A.date, y=A.percentage, hue=A.senti);\n",
    "            sns.lineplot(ax=axes[i], x=A.date, y=A.percentage, hue=A.senti, palette=['r','b','g']);\n",
    "            axes[i].set_ylabel('Percentage', fontsize=20)\n",
    "            axes[i].set_xlabel('Date', fontsize=20)\n",
    "            axes[i].set(ylim=(0,100))\n",
    "            axes[i].legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0., fontsize=20)\n",
    "            axes[i].tick_params(axis = 'both', which = 'major', labelsize = 12, rotation=45)\n",
    "\n",
    "\n",
    "    def brand_senti_percentage(self):\n",
    "        \"\"\"\n",
    "        sentiment percentage for each brand\n",
    "        \n",
    "        \"\"\"\n",
    "        sen = self.df\n",
    "\n",
    "        #get senti percentage\n",
    "        sen_group = sen.groupby(['brand','senti']).count()\n",
    "        sen = sen_group.groupby(level=(0)).apply(lambda x: x*100/x.sum())['date'].reset_index()\n",
    "        sen = sen.rename(columns={'date':'percentage'})\n",
    "\n",
    "\n",
    "        fig, ax=plt.subplots(figsize=(8,6))    \n",
    "        sns.barplot(x=sen.brand,y=sen.percentage, hue=sen.senti,ax=ax,\n",
    "                    palette={'negative':'#E60965','positive':'#77D970',\n",
    "                                                          'neutral':'#84DFFF'})\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xlabel('Sentiment',fontsize=15)\n",
    "        ax.set_ylabel('Percentage',fontsize=15)\n",
    "        plt.legend(bbox_to_anchor=(1.01,1),fontsize=12)\n",
    "        plt.title('Percentage of Sentiment Count for each Brand', fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "    def brand_count(self):\n",
    "        \"\"\"\n",
    "        count the number of tweets for each brand\n",
    "        \"\"\"\n",
    "        sent_df = self.df\n",
    "        df_counted = sent_df.groupby('brand').count()\n",
    "        x = df_counted.text.index #brands x\n",
    "        y = df_counted.text.values\n",
    "\n",
    "        p = sns.barplot(x=x,y=y,data=df_counted, palette={'AstraZeneca':'#77ACF1','Moderna':'#F29191',\n",
    "                                                          'Pfizer':'#F6D860'})\n",
    "        p.set_title('Total Tweets Number for each Brand', fontsize=20)\n",
    "        p.set_xlabel('Brands', fontsize=15)\n",
    "        p.set_ylabel('Count', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "    def each_brand_count_side_by_side(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        compare the number of tweets in percentage for each brand over time\n",
    "        \n",
    "        \"\"\"\n",
    "        sent_df = self.df\n",
    "\n",
    "        #group by date and brand, count number of tweets\n",
    "        grouped = sent_df.groupby(['date','brand']).count()\n",
    "\n",
    "        #get percentage by grouping again using level\n",
    "        grouped['percentage'] = grouped.groupby(level=0)['senti'].apply(lambda x: x*100/x.sum())                                                      \n",
    "        sent_df = grouped.reset_index()\n",
    "\n",
    "        brandcolor = [(\"AstraZeneca\",'#F29191'),('Moderna','#77ACF1'),(\"Pfizer\",'#F6D860')]\n",
    "        fig, ax = plt.subplots(1,1,figsize=(12,8))\n",
    "\n",
    "        sns.barplot(x=sent_df.date, y=sent_df.percentage, hue=sent_df.brand, \n",
    "                    palette={'AstraZeneca':'#77ACF1','Moderna':'#F29191',\n",
    "                                                          'Pfizer':'#F6D860'})\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.set_ylabel('Percentage', fontsize=15)\n",
    "        ax.set_xlabel('Date', fontsize=15)\n",
    "        plt.legend(bbox_to_anchor=(1.005,1), fontsize=12)\n",
    "        plt.title('Percentage of Total Tweet Count for each Brand', fontsize=20)\n",
    "\n",
    "\n",
    "    def ngram_bar(self, brand,num, senti=None, date=None, words_to_remove=[]): #strings\n",
    "        \"\"\"\n",
    "        show bigram trigram freq in bar chart\n",
    "        num= n most common\n",
    "        words_to_remove=[]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        sent_df = self.df\n",
    "        string = ''\n",
    "        rm=words_to_remove\n",
    "\n",
    "        if date != None:\n",
    "            tweet_N=sent_df[(sent_df['brand'] == f'{brand}')&(sent_df['text'].notnull())&\n",
    "                           (sent_df['date']==date)].text.values\n",
    "            if senti != None:\n",
    "                tweet_N=sent_df[(sent_df['brand'] == f'{brand}')&(sent_df['text'].notnull())&\n",
    "                   (sent_df['date']==date)&(sent_df['senti']==senti)].text.values\n",
    "        elif date == None:\n",
    "            tweet_N=sent_df[(sent_df['brand'] == f'{brand}')&(sent_df['text'].notnull())].text.values\n",
    "            if senti != None:\n",
    "                tweet_N=sent_df[(sent_df['brand'] == f'{brand}')&(sent_df['text'].notnull())&\n",
    "               (sent_df['date']==date)&(sent_df['senti']==senti)].text.values\n",
    "\n",
    "\n",
    "        if len(tweet_N) !=0:\n",
    "            print(len(tweet_N))\n",
    "            for sentence in tweet_N:\n",
    "                string += (sentence + ' ')\n",
    "\n",
    "            str_ls = string.split()\n",
    "            t_ls=[]\n",
    "            with_stop_ls=[]\n",
    "            for word in str_ls:\n",
    "                word = re.sub('[^a-zA-z0-9]', '',word) #only preserve alphanum for each word\n",
    "                if len(word) !=0:\n",
    "                    with_stop_ls.append(word)\n",
    "\n",
    "                    if word.lower() not in rm: #get list without stop words\n",
    "\n",
    "                            t_ls.append(word.lower())\n",
    "\n",
    "            tup = FreqDist(t_ls).most_common(num)\n",
    "            print(tup)\n",
    "\n",
    "            bgs = nltk.bigrams(t_ls) #bigram() find bigrams \n",
    "            tgs = nltk.trigrams(t_ls) #trigram() find trigrams \n",
    "            fdis2 = nltk.FreqDist(bgs) #{():count}\n",
    "            fdis3 = nltk.FreqDist(tgs) \n",
    "            tup2 = fdis2.most_common(int(num))\n",
    "            tup3 = fdis3.most_common(int(num))\n",
    "            print('\\nbigrams',tup2)\n",
    "            print('\\ntrigrams',tup3)\n",
    "\n",
    "            x2=[]\n",
    "            y2=[]\n",
    "            x3=[]\n",
    "            y3=[]\n",
    "            for t in tup2:\n",
    "                x2.append(str(t[0])) #word\n",
    "                y2.append(t[1]) #word count\n",
    "            for t in tup3:\n",
    "                x3.append(str(t[0])) #word\n",
    "                y3.append(t[1]) #word count\n",
    "            \n",
    "            if date != None:\n",
    "                thedate = f'on {date}'\n",
    "            else:\n",
    "                thedate=''\n",
    "            fig,axes = plt.subplots(2,1,figsize = (15,15))\n",
    "            sns.barplot(ax=axes[0],x=y2,y=x2, palette=\"colorblind\")\n",
    "            axes[0].set_title(f'Top {num} Bigrams for Tweets for Brand {brand} {thedate}', fontsize=20)\n",
    "            axes[0].tick_params(axis = 'both', which = 'major', labelsize = 15)\n",
    "\n",
    "            sns.barplot(ax=axes[1],x=y3,y=x3, palette=\"colorblind\")\n",
    "            axes[1].set_title(f'Top {num} Trigrams for Tweets for Brand {brand} {thedate}', fontsize=20)\n",
    "            #ax.set_ylabel(fontsize=20)\n",
    "            axes[1].tick_params(axis = 'both', which = 'major', labelsize = 15)\n",
    "            plt.show()\n",
    "            \n",
    "    def wordcloud(self, brand, words_to_remove=[]):\n",
    "        \"\"\"\n",
    "        WordCloud\n",
    "        \n",
    "        num = n most common\n",
    "        brand: \"Moderna\", \"AstraZeneca\",\"Pfizer\"\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        sentiments=['positive','negative','neutral']\n",
    "        sent_df = self.df\n",
    "\n",
    "        #words_to_remove.append(f'{brand.lower()}')\n",
    "        wls=[]\n",
    "\n",
    "        colormapls=['summer','gist_heat','winter']\n",
    "\n",
    "        stopwordset = set(words_to_remove)\n",
    "\n",
    "        \"\"\"  if words_to_remove != None:\n",
    "            stopwordset=set(words_to_remove)\n",
    "        elif words_to_remove == None:\n",
    "            stopwordset=set([])\"\"\"\n",
    "\n",
    "        for i in range(len(sentiments)): #for each sentiment in a brand\n",
    "            string = ''\n",
    "            sentiment = sentiments[i]\n",
    "            tweet_N=sent_df[(sent_df['brand'] == f'{brand}')&(sent_df['text'].notnull()) & \n",
    "                            (sent_df['senti']==sentiment)].text.values\n",
    "            if len(tweet_N) !=0:\n",
    "                for sentence in tweet_N:  #get all sentences as a string for this sentiment\n",
    "                    string += (sentence + ' ')\n",
    "\n",
    "                w = WordCloud(width = 800, height =800,\n",
    "                                background_color ='white',\n",
    "                                colormap=colormapls[i],\n",
    "                                stopwords=stopwordset,\n",
    "                                margin=20,\n",
    "                                min_font_size = 10,\n",
    "                                max_words=150).generate(string)\n",
    "\n",
    "                wls.append(w) \n",
    "\n",
    "        fig, ax = plt.subplots(nrows=len(sentiments), figsize=(24,24), facecolor = None) \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(len(sentiments)):\n",
    "            ax[i].imshow(wls[i]) #w1,w2,w3\n",
    "            ax[i].axis(\"off\")\n",
    "            ax[i].set_title(f'{brand}: {sentiments[i]} tweets', fontsize=30)\n",
    "            plt.tight_layout(pad = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c125b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = nltk.corpus.stopwords.words(\"english\") \n",
    "viz.wordcloud('Moderna', words_to_remove=rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038c597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
